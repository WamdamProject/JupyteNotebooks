{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3.2: What agriculture water use data to use for a demand site?\n",
    "\n",
    "#### By Adel M. Abdallah, Utah State University, August 2018\n",
    "\n",
    "This notebook demonstrates basic WaMDaM use cases analysis using scientific Python libraries such as [pandas](https://pandas.pydata.org/) and [plotly](https://plot.ly/).  It reads WaMDaM SQLite data, runs SQL script, and them uses Python plotly to visualize the results\n",
    "\n",
    "This use case identifies five time series and seasonal flow data for the site below Stewart Dam, Idaho\n",
    "\n",
    "\n",
    "Execute the following cells by pressing `Shift-Enter`, or by pressing the play button \n",
    "<img style='display:inline;padding-bottom:15px' src='play-button.png'>\n",
    "on the toolbar above.\n",
    "\n",
    "\n",
    "\n",
    "### Steps to reproduce this use case results and plots \n",
    "\n",
    "1.[Import python libraries](#Import)   \n",
    "   \n",
    "   \n",
    "2.[Connect to the WaMDaM populated SQLite file](#Connect)    \n",
    " \n",
    " \n",
    "3.[Query WaMDaM database for seasonal agriculture demand](#QueryDemandSeasonal)   \n",
    "  \n",
    "  \n",
    "4.[Plot the seasonal demand sites](#PlotSeasonal_a)  \n",
    "\n",
    " \n",
    "5.[Query seasonal and time series for comparisons](#QuerySeasonalTime)  \n",
    "\n",
    "\n",
    "6.[Query seasonal and time series for comparisons](#QuerySeasonalTime2)  \n",
    "\n",
    "\n",
    "7.[Plot annual demand for Cache County](#PlotAnnual)  \n",
    "\n",
    "\n",
    "8.[Pick a flow source and update the db to \"Verified\"](#PickaSource)  \n",
    "\n",
    "\n",
    "9.[Close the SQLite and WEAP API connections](#Close)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Import\"></a>\n",
    "# 1. Import python libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The needed Python libraries have been imported\n"
     ]
    }
   ],
   "source": [
    "# 1. Import python libraries \n",
    "### set the notebook mode to embed the figures within the cell\n",
    "\n",
    "import plotly\n",
    "plotly.__version__\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "offline.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=True)         # initiate notebook for offline plot\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image, SVG, Math, YouTubeVideo\n",
    "import urllib\n",
    "\n",
    "print 'The needed Python libraries have been imported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Connect\"></a>\n",
    "# 2. Connect to the WaMDaM populated SQLite file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Then we can run queries against it within this notebook :)  \n",
    "\n",
    "# the SQLite file is published here \n",
    "\n",
    "\n",
    "WaMDaM_SQLite_Name='BearRiverDatasets_June_2018_Final.sqlite'\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(WaMDaM_SQLite_Name)\n",
    "\n",
    "print 'Connected to the WaMDaM SQLite file called'+': '+ WaMDaM_SQLite_Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"QueryDemandSeasonal\"></a>\n",
    "# 3. Query WaMDaM database for seasonal + time series agriculture demand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Query WaMDaM dababase for seasonal agriculture demand\n",
    "\n",
    "# Use Case 3.1Identify_aggregate_TimeSeriesValues.csv\n",
    "# plot aggregated to monthly and converted to acre-feet time series data of multiple sources\n",
    "\n",
    "\n",
    "# 2.2Identify_aggregate_TimeSeriesValues.csv\n",
    "Query_UseCase3_2_URL=\"\"\"\n",
    "https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/4_Queries_SQL/UseCase3/UseCase3.2/2_IdentifyDemandSites_Seasonal_Values.sql\n",
    "\n",
    "\"\"\"\n",
    "# Read the query text inside the URL\n",
    "Query_UseCase3_2_text = urllib.urlopen(Query_UseCase3_2_URL).read()\n",
    "\n",
    "# print Query_UseCase3_2_text\n",
    "\n",
    "# return query result in a pandas data frame\n",
    "result_df_UseCase3_2= pd.read_sql_query(Query_UseCase3_2_text, conn)\n",
    "\n",
    "# uncomment the below line to see the list of attributes\n",
    "# display (result_df_required)\n",
    "\n",
    "\n",
    "# Save the datafrom as a csv file into the Jupyter notebook working space\n",
    "result_df_UseCase3_2.to_csv('UseCases_Results_csv\\UseCase3_2aaaaaaaaaaaa.csv', index = False)\n",
    "\n",
    "df_Seasonal=result_df_UseCase3_2\n",
    "\n",
    "# display (df_Seasonal)\n",
    "\n",
    "\n",
    "column_name = \"InstanceName\"\n",
    "subsets = df_Seasonal.groupby(column_name)\n",
    "\n",
    "\n",
    "\n",
    "print \"Queries are done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"QueryDemandSeasonal2\"></a>\n",
    "# 4. Query WaMDaM dababase for seasonal + time series agriculture demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query seasonal and time series for comparisons\n",
    "\n",
    "# Use Case 3.1Identify_aggregate_TimeSeriesValues.csv\n",
    "# plot aggregated to monthly and converted to acre-feet time series data of multiple sources\n",
    "\n",
    "\n",
    "# 2.2Identify_aggregate_TimeSeriesValues.csv\n",
    "Query_UseCase3_2_URL=\"\"\"\n",
    "https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/4_Queries_SQL/UseCase3/UseCase3.2/3_IdentifyDemandSites_TimeSeriesValues.sql\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Read the query text inside the URL\n",
    "Query_UseCase3_2_text = urllib.urlopen(Query_UseCase3_2_URL).read()\n",
    "\n",
    "\n",
    "# return query result in a pandas data frame\n",
    "result_df_UseCase3_2= pd.read_sql_query(Query_UseCase3_2_text, conn)\n",
    "\n",
    "# uncomment the below line to see the list of attributes\n",
    "# display (result_df_required)\n",
    "\n",
    "\n",
    "# Save the datafrom as a csv file into the Jupyter notebook working space\n",
    "result_df_UseCase3_2.to_csv('UseCase3_2.csv', index = False)\n",
    "\n",
    "\n",
    "######################\n",
    "# read the input data from GitHub csv file which is a direct query output for these queries:\n",
    "\n",
    "df=result_df_UseCase3_2\n",
    "\n",
    "# display (df)\n",
    "\n",
    "\n",
    "\n",
    "# Use Case 3.1Iccdentify_aggregate_TimeSeriesValues.csv\n",
    "# plot aggregated to monthly and converted to acre-feet time series data of multiple sources\n",
    "\n",
    "\n",
    "# 2.2Identify_aggregate_TimeSeriesValues.csv\n",
    "Query_UseCase3_2c_URL=\"\"\"\n",
    "https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/4_Queries_SQL/UseCase3/UseCase3.2/2_IdentifyDemandSites_Seasonal_Numeric_totals.sql\n",
    "\"\"\"\n",
    "\n",
    "# Read the query text inside the URL\n",
    "Query_UseCase3_2c_text = urllib.urlopen(Query_UseCase3_2c_URL).read()\n",
    "\n",
    "\n",
    "# return query result in a pandas data frame\n",
    "result_df_UseCase3_2c= pd.read_sql_query(Query_UseCase3_2c_text, conn)\n",
    "\n",
    "# uncomment the below line to see the list of attributes\n",
    "# display (result_df_required)\n",
    "\n",
    "\n",
    "# Save the datafrom as a csv file into the Jupyter notebook working space\n",
    "result_df_UseCase3_2c.to_csv('UseCase3_2cc.csv', index = False)\n",
    "\n",
    "\n",
    "######################\n",
    "# read the input data from GitHub csv file which is a direct query output for these queries:\n",
    "\n",
    "dfc=result_df_UseCase3_2c\n",
    "\n",
    "display (dfc)\n",
    "\n",
    "\n",
    "# DataFrame.get_value(index, col, takeable=False)[source]\n",
    "\n",
    "USU_WEAP_Model_2017_Numeric=dfc.get_value(1,'TotalNumeric')\n",
    "# print USU_WEAP_Model_2017_Numeric\n",
    "\n",
    "USU_WEAP_Model_2017_Seasonal=dfc.get_value(2,'TotalAnnualUseCacheCanals')\n",
    "# print USU_WEAP_Model_2017_Seasonal\n",
    "\n",
    "USU_WEAP_Model_2017=USU_WEAP_Model_2017_Seasonal+ USU_WEAP_Model_2017_Numeric\n",
    "\n",
    "print USU_WEAP_Model_2017\n",
    "\n",
    "print \"Queries are done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"PlotAnnual\"></a>\n",
    "# 5. Plot annual demand for Cache County\n",
    "\n",
    "\n",
    "#### Reproduce this plot [Figure 14] in the WaMDaM paper \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UseCase2.2_dentifyDemandSites_TimeSeriesValues.py\n",
    "\n",
    "# plot time series data aggregated in space and time from multiple sources\n",
    "\n",
    "\n",
    "#6.3dentifyDemandSites_TimeSeriesValues.csv\n",
    "\n",
    "## read the input data from GitHub csv file which is a direct query output\n",
    "#To get the data block (WaterYear,CumulativeAnnual) for each curve, you need to look up two columns:\n",
    "#ScenarioName and then AttributeName. So the combination of these two columns will have their separate set of data.\n",
    "\n",
    "subsets = df.groupby('AttributeName')\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "# for each subset (curve), set up its legend and line info manually so they can be edited\n",
    "subsets_settings = {\n",
    "    \n",
    "    'Diversions /surface water': {\n",
    "        'dash': 'solid',\n",
    "        'legend_index': 0,\n",
    "        'legend_name': '<br> 1 site (annual): WaDE <br> \"Diversion\"',\n",
    "        'width':3,\n",
    "                'color':'rgb(41, 10, 216)'\n",
    "\n",
    "        },    \n",
    "    \n",
    "        '8 sites (Seasonal+ avge annual): WEAP Model 2017': {\n",
    "        'dash': 'solid',\n",
    "        'legend_index': 1,\n",
    "        'legend_name': '<br> 10 sites (seasonal+ avg annual): <br> WEAP Model 2017 \"Monthly Demand\" ',\n",
    "        'width':3,\n",
    "        'color':'rgb(38, 77, 255)'        \n",
    "        },  \n",
    "    \n",
    "        \n",
    "    'Water Use /surface and ground': {\n",
    "        'dash': 'dash',\n",
    "        'legend_index': 2,\n",
    "        'legend_name': '<br> 1 site (annual): WaDE  <br> \"Water Use\"',\n",
    "        'width':3,\n",
    "        'color':'rgb(63, 160, 255)'\n",
    "        },\n",
    "    \n",
    "    'dReq': { # this one is the name of subset as it appears in the csv file\n",
    "        'dash': 'solid',     # this is properity of the line (curve)\n",
    "        'legend_index': 3,   # to order the legend\n",
    "        'legend_name': '<br> 7 sites (monthy): WASH Model <br> \"dReq\"',  # this is the manual curve name \n",
    "         'width':3,\n",
    "        'color':'rgb(114, 217, 255)'\n",
    "        },\n",
    "    'Monthly Demand_TS': {\n",
    "        'dash': 'solid',\n",
    "        'legend_index': 4,\n",
    "        'legend_name': '<br> 1 site (monthly): WEAP Model 2010 <br> \"Monthly Demand\"',\n",
    "        'width':3,\n",
    "        'color':'rgb(170, 247, 255)'\n",
    "        },\n",
    "\n",
    "            }\n",
    "    \n",
    "\n",
    "# This dict is used to map legend_name to original subset name\n",
    "subsets_names = {y['legend_name']: x for x,y in subsets_settings.iteritems()}\n",
    "\n",
    "\n",
    "\n",
    "for subset in subsets.groups.keys():\n",
    "    scenario_name_data = subsets.get_group(name=subset)\n",
    "    subsets_of_scenario = scenario_name_data.groupby(\"AttributeName\")\n",
    "    for group in subsets_of_scenario.groups.keys():\n",
    "        print group\n",
    "      \n",
    "        s = go.Scatter(\n",
    "                x = subsets_of_scenario.get_group(name=group).WaterYear[-10:],\n",
    "                y = subsets_of_scenario.get_group(name=group).CumulativeAnnual[-10:],\n",
    "                name = subsets_settings[subset]['legend_name'],\n",
    "                line = dict(\n",
    "                        color =subsets_settings[subset]['color'],\n",
    "                        width =subsets_settings[subset]['width'], \n",
    "                        dash=subsets_settings[subset]['dash']\n",
    "                        ),\n",
    "                mode = 'lines',\n",
    "                opacity = 1)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        data.append(s)\n",
    "\n",
    "        \n",
    "               \n",
    "        \n",
    "        \n",
    "# horizontal line\n",
    "horizontal_line = go.Scatter(\n",
    "    x=[2003, 2016],\n",
    "    y=[USU_WEAP_Model_2017,USU_WEAP_Model_2017],\n",
    "    mode='lines',\n",
    "    name = '<br> 10 sites (seasonal+ avg annual): <br> WEAP Model 2017 \"Monthly Demand\" ',\n",
    "#     hoverinfo='8 sites: WEAP Model 2017',\n",
    "    showlegend=True,\n",
    "    line=dict(\n",
    "        shape='hv',\n",
    "        color = 'rgb(38, 77, 255)',\n",
    "        width=3\n",
    "    )\n",
    ")\n",
    "data.append(horizontal_line)\n",
    "print USU_WEAP_Model_2017\n",
    "\n",
    "# Legend is ordered based on data, so we are sorting the data based \n",
    "# on desired legend order indicarted by the index value entered above\n",
    "data.sort(key=lambda x: subsets_settings[subsets_names[x['name']]]['legend_index'])        \n",
    "   \n",
    "layout = dict(\n",
    "    #title = \"Use Case 6\",\n",
    "    yaxis = dict(\n",
    "        title = \"Total volume per water year <br> (acre-feet)\",       \n",
    "        tickformat= ',',\n",
    "        showline=True,\n",
    "        range = ['0', '300000'],\n",
    "\n",
    "                ),\n",
    "    xaxis = dict(\n",
    "        range = ['2004', '2016'],\n",
    "        ticks='outside',\n",
    "        tickwidth=0.5,\n",
    "        ticklen=25,\n",
    "        showline=True\n",
    "                ),\n",
    "    legend=dict(x=0.86,y=0.445,\n",
    "                bordercolor='#00000f',\n",
    "                borderwidth=2  \n",
    "               ),\n",
    "     width=1650,\n",
    "    height=1000,\n",
    "    margin=go.Margin(\n",
    "        l=250,\n",
    "        b=100       ),\n",
    "    #paper_bgcolor='rgb(233,233,233)',\n",
    "    #plot_bgcolor='rgb(233,233,233)',\n",
    "    font=dict(size=32)\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# see the label Lines with Annotations\n",
    "# https://plot.ly/python/line-charts/      \n",
    "# annotations = []\n",
    "\n",
    "# label = ['8 sites', '1 site', '1 site', '7 sites','1 site']\n",
    "\n",
    "# for legend_index in subsets_settings:\n",
    "#     annotations.append(dict(xref='paper', x=2004, y=subsets_settings[legend_index],\n",
    "#                                   xanchor='right', yanchor='middle',\n",
    "#                                   text=label)\n",
    "#                       )\n",
    "\n",
    "# layout['annotations'] = annotations\n",
    "                \n",
    "\n",
    "# create a figure object          \n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "offline.iplot(fig,filename = 'Use Case 3.2-b')#,image='png' )       \n",
    "\n",
    "\n",
    "#py.iplot(fig, filename = \"UseCase2.2_dentifyDemandSites_TimeSeriesValues\") \n",
    "\n",
    "## it can be run from the local machine on Pycharm like this like below\n",
    "## It would also work here offline but in a seperate window  \n",
    "# plotly.offline.plot(fig, filename = \"UseCase2.2_dentifyDemandSites_TimeSeriesValues\")       \n",
    "\n",
    "# offline.iplot(fig,filename = 'UseCase2.2_dentifyDemandSites_TimeSeriesValues',\n",
    "#              image='png')\n",
    "# it might take 30-60 seconds to load the html interactive image \n",
    "print \"the plot is generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pick a flow source and update the db to \"Verified\"\n",
    "<a name=\"PickaSource\"></a>\n",
    "\n",
    "This \"Update\" SQL query allows users to update the Mappings table to indicate a \"verified\" DataValue. \n",
    "A verified record set to True indicates that the user has verified, curated, checked, or selected this \n",
    "data value as ready to be used for models. A verified recored can then be used from an automated script to \n",
    "serve data to models. Its particularly useful when the same set of controlled object type, attribute, and instances names \n",
    "return multiple data values from different sources with potentially smiliar or different values due to many factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scenario_name_data = subsets.get_group(name='Base case')\n",
    "# print scenario_name_data\n",
    "# Get a cursor object\n",
    "\n",
    "SQL_update = \"\"\"\n",
    "UPDATE Mappings \n",
    "\n",
    "SET Verified= 'True'\n",
    "WHERE  MappingID in\n",
    "\n",
    "(\n",
    "\n",
    "SELECT Mappings.MappingID FROM Mappings\n",
    "\n",
    "-- Join the Mappings to get their Attributes\n",
    "LEFT JOIN \"Attributes\"\n",
    "ON Attributes.AttributeID= Mappings.AttributeID\n",
    "\n",
    "-- Join the Attributes to get their ObjectTypes\n",
    "LEFT JOIN  \"ObjectTypes\"\n",
    "ON \"ObjectTypes\".\"ObjectTypeID\"=\"Attributes\".\"ObjectTypeID\"\n",
    "\n",
    "-- Join the Mappings to get their Instances   \n",
    "LEFT JOIN \"Instances\" \n",
    "ON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n",
    "\n",
    "-- Join the Mappings to get their ScenarioMappings   \n",
    "LEFT JOIN \"ScenarioMappings\"\n",
    "ON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n",
    "\n",
    "-- Join the ScenarioMappings to get their Scenarios   \n",
    "LEFT JOIN \"Scenarios\"\n",
    "ON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n",
    "\n",
    "-- Join the Scenarios to get their MasterNetworks   \n",
    "LEFT JOIN \"MasterNetworks\" \n",
    "ON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n",
    "\n",
    "where \n",
    "\n",
    "ObjectTypes.ObjectType='AGRICULTURE'  \n",
    "\n",
    "\n",
    "AND \"Instances\".\"InstanceName\"='Cache Valley ag'  \n",
    "\n",
    "\n",
    "AND AttributeName='Water Use /surface and ground'\n",
    "\n",
    "\n",
    "AND ScenarioName='Utah Data'\n",
    "\n",
    "AND MasterNetworkName='Western States Water Use'\n",
    "\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "res = cur.execute(SQL_update)\n",
    "\n",
    "print 'Updated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Close\"></a>\n",
    "# 7. Close the SQLite connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()\n",
    "\n",
    "print 'Connection to SQLite engine is disconnected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End :) Congratulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
