{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaMDaM_Use_Case 3.1: What flow values to use at a site (e.g., below Steward Dam)? \n",
    "\n",
    "#### By Adel M. Abdallah, Utah State University, August 2018\n",
    "\n",
    "This notebook demonstrates basic WaMDaM use cases analysis using scientific Python libraries such as [pandas](https://pandas.pydata.org/) and [plotly](https://plot.ly/).  It reads WaMDaM SQLite data from a published HydroShare Generic Resource, runs SQL script, and them uses Python plotly to visualize the results\n",
    "\n",
    "This use case identifies five time series and seasonal flow data for the site below Stewart Dam, Idaho\n",
    "\n",
    "\n",
    "\n",
    "Execute the following cells by pressing `Shift-Enter`, or by pressing the play button \n",
    "<img style='display:inline;padding-bottom:15px' src='play-button.png'>\n",
    "on the toolbar above.\n",
    "\n",
    "\n",
    "### Steps to reproduce this use case results and plots \n",
    "\n",
    "1.[Import python libraries](#Import)   \n",
    "   \n",
    "   \n",
    "2.[Connect to the WaMDaM populated SQLite file](#Connect)    \n",
    " \n",
    " \n",
    "3.[Query WaMDaM database for flow time series](#QueryFlowTimeSeries)   \n",
    "  \n",
    "  \n",
    "4.[Plot the compiled time series for Stewart Dam (Figure 11-A)](#PlotFlow12A)  \n",
    " \n",
    " \n",
    "5.[Plot the last 15 years to show discrepancy in time series for Stewart Dam (Figure 12-B))](#PlotFlow12B)  \n",
    " \n",
    " \n",
    "6.[Pick a a flow source and update the WaMDaM db to reflect \"Verified\"](#PickaSource)  \n",
    " \n",
    " \n",
    "7.[Close the SQLite and WEAP API connections](#Close)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Import\"></a>\n",
    "# 1. Import python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The needed Python libraries have been imported\n"
     ]
    }
   ],
   "source": [
    "# 1. Import python libraries \n",
    "### set the notebook mode to embed the figures within the cell\n",
    "\n",
    "import plotly\n",
    "plotly.__version__\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "offline.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=True)         # initiate notebook for offline plot\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image, SVG, Math, YouTubeVideo\n",
    "import urllib\n",
    "\n",
    "print 'The needed Python libraries have been imported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Connect\"></a>\n",
    "# 2. Connect to the WaMDaM populated SQLite file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the WaMDaM SQLite file called: BearRiverDatasets_June_2018_Final.sqlite\n"
     ]
    }
   ],
   "source": [
    "# 2. Connect to the WaMDaM populated SQLite file \n",
    "\n",
    "\n",
    "# Then we can run queries against it within this notebook :)  \n",
    "\n",
    "# the SQLite file is published here \n",
    "#https://github.com/WamdamProject/WaMDaM_UseCases/blob/master/UseCases_files/3SQLite_database/BearRiverDatasets_June_2018.sqlite\n",
    "\n",
    "WaMDaM_SQLite_Name='BearRiverDatasets_June_2018_Final.sqlite'\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('WaMDaM_SQLite_Name')\n",
    "\n",
    "print 'Connected to the WaMDaM SQLite file called'+': '+ WaMDaM_SQLite_Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"QueryFlowTimeSeries\"></a>\n",
    "# 3. Query WaMDaM database for flow time series \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '/*\n2.2Identify_aggregate_TimeSeriesValues.sql\n\nUse case 2: identify and compare time series and seasonal discharge data across data sources. \nWhat is the discharge at the node â€œbelow Stewart Damâ€� in Idaho?\n\n\nAggregate time series data from daily cfs into cumulative monthly values in acre-feet\nIf the time series is a Water Year, then convert it to a Calendar year\n\nHere the use of controlled unit names become valuable. \nUsers need to check on the unit name to perform a conversion like from cfs to af/month\n\nThe query below has two parts: \nThe first gets the daily data and aggregate it to monthly and convert its unit af \nThe second gets the monthly data and keep it monthly but convert it from cfs to af \nThe two parts are needed because of the â€œHavingâ€� function in the second part (to check on the days of the month)\n\nAdel Abdallah\nUpdated April 3, 2018\n*/\n--                                               Two select statements will be join together by UNION ALL function\n--                                    both Select Statements return identical column headers. Otherwise the Union will not work\n--*************************************************************************************************************************************************************\n\n                                                   --The first SELECT statement (get the daily data and convert to monthly)\n\n--ResourceTypeAcronym,AttributeName, InstanceName,AggregationStatisticCV,IntervalTimeUnitCV,UnitNameCV,YearType,YearMonth, TimeSeriesValueID,CountDays,CalenderYear,CumulativeMonthly\n\nSELECT ResourceTypeAcronym,AttributeName, InstanceName,AggregationStatisticCV,IntervalTimeUnitCV,UnitNameCV,\nYearType,strftime('%m/%Y', DateTimeStamp) as YearMonth, TimeSeriesValueID,count(DataValue) As CountDays,\n--convert the time stamp to be in the format of Month and Year (no days)\n\n\n--check if it is a water year by querying the field \"YearType\" in the TimeSeries table\nCase \n        WHEN YearType='WaterYear' AND (strftime('%m', DateTimeStamp) ='10' or  strftime('%m', DateTimeStamp) ='11' or  strftime('%m', DateTimeStamp) ='12')\n        -- if it is a water year, then subtract one year from the time stamps of Oct., Nov., and Dec. months in each year\n        THEN date(DateTimeStamp,'-1 year')   \n        --if not a water year, then keep the time stamp as is\n        Else DateTimeStamp \nEnd CalenderYear,\n\n\n--covert the cfs/month to cumulative acre-ft per month \n-- Divide by 43560 square feet then multiply by 60*60*24 (and 30) to convert\nCase \n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='day' AND AggregationStatisticCV='Cumulative' AND count(Datavalue)>=27)   THEN SUM(DataValue)/43560*60*60\n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='day' AND AggregationStatisticCV='Average' AND count(DataValue)>=27)   THEN SUM(DataValue)/43560*60*60*24\n         Else null\nEND As CumulativeMonthly\n\n\nFROM ResourceTypes\n\nLeft JOIN \"ObjectTypes\" \nON \"ObjectTypes\".\"ResourceTypeID\"=\"ResourceTypes\".\"ResourceTypeID\"\n\n-- Join the Objects to get their attributes  \nLEFT JOIN  \"Attributes\"\nON \"Attributes\".\"ObjectTypeID\"=\"ObjectTypes\".\"ObjectTypeID\"\n\nLEFT JOIN \"Mappings\"\nON \"Mappings\".\"AttributeID\"= \"Attributes\".\"AttributeID\"\n\nLEFT JOIN \"Instances\" \nON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n\nLEFT JOIN \"ValuesMapper\" \nON \"ValuesMapper\".\"ValuesMapperID\"=\"Mappings\".\"ValuesMapperID\"\n\nLEFT JOIN \"ScenarioMappings\"\nON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n\nLEFT JOIN \"Scenarios\" \nON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n\nLEFT JOIN \"MasterNetworks\" \nON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n\nLEFT JOIN \"TimeSeries\" \nON \"TimeSeries\".\"ValuesMapperID\"=\"ValuesMapper\".\"ValuesMapperID\"\n\nLEFT JOIN \"TimeSeriesValues\" \nON \"TimeSeriesValues\".\"TimeSeriesID\"=\"TimeSeries\".\"TimeSeriesID\"\n\n\nWHERE\nAttributeDataTypeCV='TimeSeries'\n\nAND \"InstanceNameCV\"='USGS 10046500 BEAR RIVER BL STEWART DAM NR MONTPELIER, ID'\n\nAND \"AttributeNameCV\"='Flow'\n\n--AND ResourceTypeAcronym='BearRiverCommission'\n\n\n-- Daily time series. \nAND IntervalTimeUnitCV='day'\n\nGROUP BY ResourceTypeAcronym,AttributeName,InstanceName,YearType,YearMonth\n\n-- use this only if converting from daily to monthly (otherwise, months wont show up because their count is just 1)\n--The use of \"HAVING\" clause enables you to specify conditions that filter which group results appear in the final results.\n\n-- exclude the months that have data for less than 29days\nHaving CountDays>=27\n\n\n--*************************************************************************************************************************************************************\n\n                                                                                                 UNION ALL\n\n--*************************************************************************************************************************************************************\n\n                                                   --The second SELECT statement (get the monthly data and keep it monthly)\n\nSELECT ResourceTypeAcronym,AttributeName, InstanceName,AggregationStatisticCV,IntervalTimeUnitCV,UnitNameCV,\nYearType,strftime('%m/%Y', DateTimeStamp) as YearMonth, TimeSeriesValueID,count(DataValue) As CountDays,\n\n\n--check if it is a water year by querying the field \"YearType\" in the TimeSeries table\n--convert the time stamp to be in the format of Month and Year (no days)\n-- If the time series is \"WateYear\", then convert it to a calendar year.\nCase \n        WHEN YearType='WaterYear' AND (strftime('%m', DateTimeStamp) ='10' or  strftime('%m', DateTimeStamp) ='11' or  strftime('%m', DateTimeStamp) ='12')\n        -- if it is a water year, then subtract one year from the time stamps of Oct., Nov., and Dec. months in each year\n        THEN date(DateTimeStamp,'-1 year')   \n        --if not a water year, then keep the time stamp as is\nElse DateTimeStamp \nEnd As CalenderYear,\n\n\n--covert the cfs/month to cumulative acre-ft per month \n-- Divide by 43559.9 square feet then multiply by 60*60*24\nCase \n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='month' AND AggregationStatisticCV='Cumulative') THEN DataValue/43560\n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='month' AND AggregationStatisticCV='Average' )   THEN DataValue/43560*60*60*24*30\n         Else DataValue\nEND As CumulativeMonthly\n\n\n\nFROM \"ResourceTypes\"\n\nLeft JOIN \"ObjectTypes\" \nON \"ObjectTypes\".\"ResourceTypeID\"=\"ResourceTypes\".\"ResourceTypeID\"\n\n-- Join the Objects to get their attributes  \nLEFT JOIN  \"Attributes\"\nON \"Attributes\".\"ObjectTypeID\"=\"ObjectTypes\".\"ObjectTypeID\"\n\nLEFT JOIN \"Mappings\"\nON \"Mappings\".\"AttributeID\"= \"Attributes\".\"AttributeID\"\n\nLEFT JOIN \"Instances\" \nON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n\nLEFT JOIN \"ValuesMapper\" \nON \"ValuesMapper\".\"ValuesMapperID\"=\"Mappings\".\"ValuesMapperID\"\n\nLEFT JOIN \"ScenarioMappings\"\nON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n\nLEFT JOIN \"Scenarios\" \nON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n\nLEFT JOIN \"MasterNetworks\" \nON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n\nLEFT JOIN \"TimeSeries\" \nON \"TimeSeries\".\"ValuesMapperID\"=\"ValuesMapper\".\"ValuesMapperID\"\n\nLEFT JOIN \"TimeSeriesValues\" \nON \"TimeSeriesValues\".\"TimeSeriesID\"=\"TimeSeries\".\"TimeSeriesID\"\n\n--WHERE InstanceName='BEAR RIVER BL STEWART DAM NR MONTPELIER, ID'\n\nWHERE\nAttributeDataTypeCV='TimeSeries'\n\nAND \"InstanceNameCV\"='USGS 10046500 BEAR RIVER BL STEWART DAM NR MONTPELIER, ID'\n\nAND \"AttributeNameCV\"='Flow'\n\n--AND datasetacronym='BearRiverCommission'\n\n\n-- It is best to filter by day or month values. \n--Then you can use the â€œHavingâ€� clause below for daily but need to comment it for monthly\nAND IntervalTimeUnitCV='month'\n\nGROUP BY ResourceTypeAcronym,AttributeName,InstanceName,YearType,YearMonth\n\n-- use this only if converting from daily to monthly (otherwise, months wont show up because their count is just 1)\n--The use of \"HAVING\" clause enables you to specify conditions that filter which group results appear in the final results.\n-- exclude the months that have data for less than 27 days\n--Having CountDays>=27\n\n--*************************************************************************************************************************************************************\n\nORDER BY TimeSeriesValueID ,ResourceTypeAcronym,CalenderYear,AttributeName,InstanceName ASC\n\n\n': no such table: ResourceTypes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bbf64ffeb09a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;31m# return query result in a pandas data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mresult_df_UseCase3_1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQuery_UseCase3_1_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;31m# uncomment the below line to see the list of attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    330\u001b[0m     return pandas_sql.read_query(\n\u001b[1;32m    331\u001b[0m         \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         parse_dates=parse_dates, chunksize=chunksize)\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             ex = DatabaseError(\n\u001b[1;32m   1415\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '/*\n2.2Identify_aggregate_TimeSeriesValues.sql\n\nUse case 2: identify and compare time series and seasonal discharge data across data sources. \nWhat is the discharge at the node â€œbelow Stewart Damâ€� in Idaho?\n\n\nAggregate time series data from daily cfs into cumulative monthly values in acre-feet\nIf the time series is a Water Year, then convert it to a Calendar year\n\nHere the use of controlled unit names become valuable. \nUsers need to check on the unit name to perform a conversion like from cfs to af/month\n\nThe query below has two parts: \nThe first gets the daily data and aggregate it to monthly and convert its unit af \nThe second gets the monthly data and keep it monthly but convert it from cfs to af \nThe two parts are needed because of the â€œHavingâ€� function in the second part (to check on the days of the month)\n\nAdel Abdallah\nUpdated April 3, 2018\n*/\n--                                               Two select statements will be join together by UNION ALL function\n--                                    both Select Statements return identical column headers. Otherwise the Union will not work\n--*************************************************************************************************************************************************************\n\n                                                   --The first SELECT statement (get the daily data and convert to monthly)\n\n--ResourceTypeAcronym,AttributeName, InstanceName,AggregationStatisticCV,IntervalTimeUnitCV,UnitNameCV,YearType,YearMonth, TimeSeriesValueID,CountDays,CalenderYear,CumulativeMonthly\n\nSELECT ResourceTypeAcronym,AttributeName, InstanceName,AggregationStatisticCV,IntervalTimeUnitCV,UnitNameCV,\nYearType,strftime('%m/%Y', DateTimeStamp) as YearMonth, TimeSeriesValueID,count(DataValue) As CountDays,\n--convert the time stamp to be in the format of Month and Year (no days)\n\n\n--check if it is a water year by querying the field \"YearType\" in the TimeSeries table\nCase \n        WHEN YearType='WaterYear' AND (strftime('%m', DateTimeStamp) ='10' or  strftime('%m', DateTimeStamp) ='11' or  strftime('%m', DateTimeStamp) ='12')\n        -- if it is a water year, then subtract one year from the time stamps of Oct., Nov., and Dec. months in each year\n        THEN date(DateTimeStamp,'-1 year')   \n        --if not a water year, then keep the time stamp as is\n        Else DateTimeStamp \nEnd CalenderYear,\n\n\n--covert the cfs/month to cumulative acre-ft per month \n-- Divide by 43560 square feet then multiply by 60*60*24 (and 30) to convert\nCase \n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='day' AND AggregationStatisticCV='Cumulative' AND count(Datavalue)>=27)   THEN SUM(DataValue)/43560*60*60\n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='day' AND AggregationStatisticCV='Average' AND count(DataValue)>=27)   THEN SUM(DataValue)/43560*60*60*24\n         Else null\nEND As CumulativeMonthly\n\n\nFROM ResourceTypes\n\nLeft JOIN \"ObjectTypes\" \nON \"ObjectTypes\".\"ResourceTypeID\"=\"ResourceTypes\".\"ResourceTypeID\"\n\n-- Join the Objects to get their attributes  \nLEFT JOIN  \"Attributes\"\nON \"Attributes\".\"ObjectTypeID\"=\"ObjectTypes\".\"ObjectTypeID\"\n\nLEFT JOIN \"Mappings\"\nON \"Mappings\".\"AttributeID\"= \"Attributes\".\"AttributeID\"\n\nLEFT JOIN \"Instances\" \nON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n\nLEFT JOIN \"ValuesMapper\" \nON \"ValuesMapper\".\"ValuesMapperID\"=\"Mappings\".\"ValuesMapperID\"\n\nLEFT JOIN \"ScenarioMappings\"\nON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n\nLEFT JOIN \"Scenarios\" \nON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n\nLEFT JOIN \"MasterNetworks\" \nON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n\nLEFT JOIN \"TimeSeries\" \nON \"TimeSeries\".\"ValuesMapperID\"=\"ValuesMapper\".\"ValuesMapperID\"\n\nLEFT JOIN \"TimeSeriesValues\" \nON \"TimeSeriesValues\".\"TimeSeriesID\"=\"TimeSeries\".\"TimeSeriesID\"\n\n\nWHERE\nAttributeDataTypeCV='TimeSeries'\n\nAND \"InstanceNameCV\"='USGS 10046500 BEAR RIVER BL STEWART DAM NR MONTPELIER, ID'\n\nAND \"AttributeNameCV\"='Flow'\n\n--AND ResourceTypeAcronym='BearRiverCommission'\n\n\n-- Daily time series. \nAND IntervalTimeUnitCV='day'\n\nGROUP BY ResourceTypeAcronym,AttributeName,InstanceName,YearType,YearMonth\n\n-- use this only if converting from daily to monthly (otherwise, months wont show up because their count is just 1)\n--The use of \"HAVING\" clause enables you to specify conditions that filter which group results appear in the final results.\n\n-- exclude the months that have data for less than 29days\nHaving CountDays>=27\n\n\n--*************************************************************************************************************************************************************\n\n                                                                                                 UNION ALL\n\n--*************************************************************************************************************************************************************\n\n                                                   --The second SELECT statement (get the monthly data and keep it monthly)\n\nSELECT ResourceTypeAcronym,AttributeName, InstanceName,AggregationStatisticCV,IntervalTimeUnitCV,UnitNameCV,\nYearType,strftime('%m/%Y', DateTimeStamp) as YearMonth, TimeSeriesValueID,count(DataValue) As CountDays,\n\n\n--check if it is a water year by querying the field \"YearType\" in the TimeSeries table\n--convert the time stamp to be in the format of Month and Year (no days)\n-- If the time series is \"WateYear\", then convert it to a calendar year.\nCase \n        WHEN YearType='WaterYear' AND (strftime('%m', DateTimeStamp) ='10' or  strftime('%m', DateTimeStamp) ='11' or  strftime('%m', DateTimeStamp) ='12')\n        -- if it is a water year, then subtract one year from the time stamps of Oct., Nov., and Dec. months in each year\n        THEN date(DateTimeStamp,'-1 year')   \n        --if not a water year, then keep the time stamp as is\nElse DateTimeStamp \nEnd As CalenderYear,\n\n\n--covert the cfs/month to cumulative acre-ft per month \n-- Divide by 43559.9 square feet then multiply by 60*60*24\nCase \n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='month' AND AggregationStatisticCV='Cumulative') THEN DataValue/43560\n         WHEN (UnitNameCV='cubic feet per second' AND  IntervalTimeUnitCV='month' AND AggregationStatisticCV='Average' )   THEN DataValue/43560*60*60*24*30\n         Else DataValue\nEND As CumulativeMonthly\n\n\n\nFROM \"ResourceTypes\"\n\nLeft JOIN \"ObjectTypes\" \nON \"ObjectTypes\".\"ResourceTypeID\"=\"ResourceTypes\".\"ResourceTypeID\"\n\n-- Join the Objects to get their attributes  \nLEFT JOIN  \"Attributes\"\nON \"Attributes\".\"ObjectTypeID\"=\"ObjectTypes\".\"ObjectTypeID\"\n\nLEFT JOIN \"Mappings\"\nON \"Mappings\".\"AttributeID\"= \"Attributes\".\"AttributeID\"\n\nLEFT JOIN \"Instances\" \nON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n\nLEFT JOIN \"ValuesMapper\" \nON \"ValuesMapper\".\"ValuesMapperID\"=\"Mappings\".\"ValuesMapperID\"\n\nLEFT JOIN \"ScenarioMappings\"\nON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n\nLEFT JOIN \"Scenarios\" \nON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n\nLEFT JOIN \"MasterNetworks\" \nON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n\nLEFT JOIN \"TimeSeries\" \nON \"TimeSeries\".\"ValuesMapperID\"=\"ValuesMapper\".\"ValuesMapperID\"\n\nLEFT JOIN \"TimeSeriesValues\" \nON \"TimeSeriesValues\".\"TimeSeriesID\"=\"TimeSeries\".\"TimeSeriesID\"\n\n--WHERE InstanceName='BEAR RIVER BL STEWART DAM NR MONTPELIER, ID'\n\nWHERE\nAttributeDataTypeCV='TimeSeries'\n\nAND \"InstanceNameCV\"='USGS 10046500 BEAR RIVER BL STEWART DAM NR MONTPELIER, ID'\n\nAND \"AttributeNameCV\"='Flow'\n\n--AND datasetacronym='BearRiverCommission'\n\n\n-- It is best to filter by day or month values. \n--Then you can use the â€œHavingâ€� clause below for daily but need to comment it for monthly\nAND IntervalTimeUnitCV='month'\n\nGROUP BY ResourceTypeAcronym,AttributeName,InstanceName,YearType,YearMonth\n\n-- use this only if converting from daily to monthly (otherwise, months wont show up because their count is just 1)\n--The use of \"HAVING\" clause enables you to specify conditions that filter which group results appear in the final results.\n-- exclude the months that have data for less than 27 days\n--Having CountDays>=27\n\n--*************************************************************************************************************************************************************\n\nORDER BY TimeSeriesValueID ,ResourceTypeAcronym,CalenderYear,AttributeName,InstanceName ASC\n\n\n': no such table: ResourceTypes"
     ]
    }
   ],
   "source": [
    "# Use Case 3.1Identify_aggregate_TimeSeriesValues.csv\n",
    "# plot aggregated to monthly and converted to acre-feet time series data of multiple sources\n",
    "\n",
    "\n",
    "\n",
    "# 2.2Identify_aggregate_TimeSeriesValues.csv\n",
    "Query_UseCase3_1_URL=\"\"\"\n",
    "https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/4_Queries_SQL/UseCase3/UseCase3.1/2_Identify_aggregate_TimeSeriesValues.sql\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Read the query text inside the URL\n",
    "Query_UseCase3_1_text = urllib.urlopen(Query_UseCase3_1_URL).read()\n",
    "\n",
    "\n",
    "# return query result in a pandas data frame\n",
    "result_df_UseCase3_1= pd.read_sql_query(Query_UseCase3_1_text, conn)\n",
    "\n",
    "# uncomment the below line to see the list of attributes\n",
    "# display (result_df_required)\n",
    "\n",
    "\n",
    "# Save the datafrom as a csv file into the Jupyter notebook working space\n",
    "result_df_UseCase3_1.to_csv('UseCases_Results_csv\\UseCase3_1.csv', index = False)\n",
    "\n",
    "print \"Queries are done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"PlotFlow12A\"></a>\n",
    "# 4. Plot the compiled time series for Stewart Dam (Figure xx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Plot the compiled time series for Stewart Dam (Figure 11-A)\n",
    "\n",
    "\n",
    "df_TimeSeries=result_df_UseCase3_1\n",
    "# identify the data for four time series only based on the DatasetAcronym column header \n",
    "column_name = \"ResourceTypeAcronym\"\n",
    "subsets = df_TimeSeries.groupby(column_name)\n",
    "data = []\n",
    "\n",
    "# for each subset (curve), set up its legend and line info manually so they can be edited\n",
    "subsets_settings = {\n",
    "    'UDWRFlowData': {\n",
    "        'dash': 'solid',\n",
    "        'legend_index': 0,\n",
    "        'legend_name': 'Utah Division of Water Res.',\n",
    "        'width':3,\n",
    "        'color':'rgb(153, 15, 15)'\n",
    "        },\n",
    "    'CUAHSI': {\n",
    "        'dash': 'dash',\n",
    "        'legend_index': 1,\n",
    "        'legend_name': 'USGS',\n",
    "        'width':4,\n",
    "        'color':'rgb(15, 107, 153)'\n",
    "        },\n",
    "    'IdahoWRA': {\n",
    "        'dash': 'solid',\n",
    "        'legend_index': 2,\n",
    "        'legend_name': 'Idaho Department of Water Res.',\n",
    "        'width':3,\n",
    "        'color':'rgb(38, 15, 153)'\n",
    "        },    \n",
    "    'BearRiverCommission': { # this oone is the name of subset as it appears in the csv file\n",
    "        'dash': 'dot',     # this is properity of the line (curve)\n",
    "        'legend_index': 3,   # to order the legend\n",
    "        'legend_name': 'Bear River Commission',  # this is the manual curve name \n",
    "         'width':4,\n",
    "        'color':'rgb(107, 153, 15)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "# This dict is used to map legend_name to original subset name\n",
    "subsets_names = {y['legend_name']: x for x,y in subsets_settings.iteritems()}\n",
    "\n",
    "# prepare the scater plot for each curve\n",
    "for subset in subsets.groups.keys():\n",
    "    #print subset\n",
    "    dt = subsets.get_group(name=subset)\n",
    "    s = go.Scatter(\n",
    "                    x=dt.CalenderYear.map(lambda z: str(z)[:-3]),\n",
    "                    y=dt['CumulativeMonthly'],\n",
    "                    name = subsets_settings[subset]['legend_name'],\n",
    "                    line = dict(\n",
    "                        color =subsets_settings[subset]['color'],\n",
    "                        width =subsets_settings[subset]['width'], \n",
    "                        dash=subsets_settings[subset]['dash']\n",
    "                               ),\n",
    "                        opacity = 1                                \n",
    "                  )\n",
    "    data.append(s)\n",
    "    \n",
    "# Legend is ordered based on data, so we are sorting the data based \n",
    "# on desired legend order indicarted by the index value entered above\n",
    "data.sort(key=lambda x: subsets_settings[subsets_names[x['name']]]['legend_index'])\n",
    "\n",
    "# set up the figure layout parameters\n",
    "layout = dict(\n",
    "     #title = \"UseCase3.2\",\n",
    "     yaxis = dict(\n",
    "         title = \"Cumulative monthly flow <br> (acre-feet/month)\",\n",
    "         tickformat= ',',\n",
    "         zeroline=True,\n",
    "         showline=True,\n",
    "         ticks='outside',\n",
    "         ticklen=15,\n",
    "         #zerolinewidth=4,\n",
    "         zerolinecolor='#00000f',\n",
    "\n",
    "         dtick=30000,\n",
    "                 ),\n",
    "    xaxis = dict(\n",
    "         #title = \"Time <br> (month/year)\",\n",
    "         #autotick=False,\n",
    "        tick0='1900-01-01',\n",
    "        dtick='M180',\n",
    "        ticks='inside',\n",
    "        tickwidth=0.5,\n",
    "        #zerolinewidth=4,\n",
    "        ticklen=27,\n",
    "        zerolinecolor='#00000f',\n",
    "        tickcolor='#000',\n",
    "        tickformat= \"%Y\",\n",
    "       range = ['1920', '2020']\n",
    "\n",
    "                ),\n",
    "    legend=dict(\n",
    "        x=0.2,y=0.9,\n",
    "        bordercolor='#00000f',\n",
    "            borderwidth=2\n",
    "\n",
    "\n",
    "                ),\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    margin=go.Margin(l=300, b=150),\n",
    "    #paper_bgcolor='rgb(233,233,233)',\n",
    "    #plot_bgcolor='rgb(233,233,233)',\n",
    "    \n",
    "    \n",
    "    font=dict( size=35)\n",
    "             )\n",
    "# create the figure object            \n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "# plot the figure \n",
    "offline.iplot(fig,filename = 'UseCase3.1a_TimeSeries')#,image='png' )       \n",
    "\n",
    "\n",
    "## it can be run from the local machine on Pycharm like this like below\n",
    "## It would also work here offline but in a seperate window  \n",
    "\n",
    "#plotly.offline.plot(fig, filename = \"2.2Identify_aggregate_TimeSeriesValues.html\") \n",
    "\n",
    "###########################################################################################################\n",
    "# Have you encounterd the messages below? if not, dont worry about it\n",
    "# ----------------------------------------------\n",
    "# Javascript error adding output!\n",
    "# ReferenceError: Plotly is not defined\n",
    "# See your browser Javascript console for more details.\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Do the follwoing:\n",
    "\n",
    "# Kernel -> Restart -> Clear all outputs and restart\n",
    "# Save\n",
    "# Close browser\n",
    "# Open browser and run again\n",
    "\n",
    "print \"the plot is generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"PlotFlow12B\"></a>\n",
    "# 5. Plot the last 15 years to show discrepency in time series for Stewart Dam (Figure xx-B)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Plot the last 15 years to show discrepency in time series for Stewart Dam (Figure 12-b)\n",
    "\n",
    "# Use Case 2.2bIdentify_aggregate_TimeSeriesValues.py\n",
    "# plot aggregated to monthly and converted to acre-feet time series data of multiple sources\n",
    "\n",
    "# Adel Abdallah\n",
    "# November 16, 2017\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "\n",
    "## read the input data from GitHub csv file which is a direct query output for this  query:\n",
    "# 3.2Identify_aggregate_TimeSeriesValues.sql\n",
    "\n",
    "\n",
    "# identify the data for four time series only based on the DatasetAcronym column header \n",
    "column_name = \"ResourceTypeAcronym\"\n",
    "subsets = df_TimeSeries.groupby(column_name)\n",
    "data = []\n",
    "\n",
    "# for each subset (curve), set up its legend and line info manually so they can be edited\n",
    "\n",
    "subsets_settings = {\n",
    "    'UDWRFlowData': {\n",
    "        'symbol': \"star\",\n",
    "        'legend_index': 0,\n",
    "        'legend_name': 'Utah Division of Water Res.',\n",
    "        'width':2,\n",
    "        'size' :7,\n",
    "        'color':'rgb(153, 15, 15)',\n",
    "        'mode': 'lines+markers'\n",
    "        },\n",
    "    'CUAHSI': {\n",
    "        'symbol': \"square\",\n",
    "        'legend_index': 1,\n",
    "         'size' :10,\n",
    "        'legend_name': 'CUAHSI',\n",
    "        'width':3,\n",
    "        'color':'rgb(15, 107, 153)',\n",
    "        'show_legend': False,\n",
    "        },\n",
    "    'IdahoWRA': {\n",
    "        'symbol': \"triangle-down\",\n",
    "        'legend_index': 2,\n",
    "         'size' :6,\n",
    "        'legend_name': 'Idaho Department of Water Res.',\n",
    "        'width':3,\n",
    "        'color':'rgb(38, 15, 153)'\n",
    "        },    \n",
    "    'BearRiverCommission': { # this one is the name of subset as it appears in the csv file\n",
    "        'symbol': 106,     # this is property of the line (curve)\n",
    "                'size' :6,\n",
    "\n",
    "        'legend_index': 3,   # to order the legend\n",
    "        'legend_name': \"Bear River Commission\",  # this is the manual curve name \n",
    "         'width':4,\n",
    "        'color':'rgb(107, 153, 15)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "# This dict is used to map legend_name to original subset name\n",
    "subsets_names = {y['legend_name']: x for x,y in subsets_settings.iteritems()}\n",
    "\n",
    "# prepare the scater plot for each curve\n",
    "for subset in subsets.groups.keys():\n",
    "    print subset\n",
    "    dt = subsets.get_group(name=subset)\n",
    "    s = go.Scatter(\n",
    "        x=dt.CalenderYear.map(lambda z: str(z)[:-3]),\n",
    "        y=dt['CumulativeMonthly'],\n",
    "        name = subsets_settings[subset]['legend_name'],       \n",
    "        opacity = 1,\n",
    "        \n",
    "        # Get mode from settings dictionary, if there is no mode\n",
    "        # defined in dictinoary, then default is markers.\n",
    "        mode = subsets_settings[subset].get('mode', 'markers'),\n",
    "        \n",
    "        # Get legend mode from settings dictionary, if there is no mode\n",
    "        # defined in dictinoary, then default is to show item in legend.\n",
    "        showlegend = subsets_settings[subset].get('show_legend', True),\n",
    "        \n",
    "        marker = dict(\n",
    "            size =subsets_settings[subset]['size'],\n",
    "            color = '#FFFFFF',      # white\n",
    "            symbol =subsets_settings[subset]['symbol'],\n",
    "            line = dict(\n",
    "                color =subsets_settings[subset]['color'],\n",
    "                width =subsets_settings[subset]['width'], \n",
    "                ),\n",
    "            ),\n",
    "            \n",
    "        line = dict(\n",
    "            color =subsets_settings[subset]['color'],\n",
    "            width =subsets_settings[subset]['width'], \n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    data.append(s)\n",
    "    \n",
    "# Legend is ordered based on data, so we are sorting the data based \n",
    "# on desired legend order indicated by the index value entered above\n",
    "data.sort(key=lambda x: subsets_settings[subsets_names[x['name']]]['legend_index'])\n",
    "\n",
    "# set up the figure layout parameters\n",
    "layout = dict(\n",
    "     #title = \"UseCase3.2\",\n",
    "     yaxis = dict(\n",
    "         title = \"Cumulative monthly flow <br> (acre-feet/month)\",\n",
    "         tickformat= ',',\n",
    "         zeroline=True,\n",
    "         showline=True,\n",
    "         ticks='outside',\n",
    "         ticklen=15,\n",
    "         #zerolinewidth=4,\n",
    "         zerolinecolor='#00000f',\n",
    "         range = ['0', '6000'],\n",
    "         dtick=1000,\n",
    "                 ),\n",
    "    xaxis = dict(\n",
    "         #title = \"Time <br> (month/year)\",\n",
    "         #autotick=False,\n",
    "        tick0='1994-01-01',\n",
    "        showline=True,\n",
    "        dtick='M12',\n",
    "        ticks='outside',\n",
    "        tickwidth=0.5,\n",
    "        #zerolinewidth=4,\n",
    "        ticklen=27,\n",
    "        #zerolinecolor='#00000',\n",
    "        tickcolor='#000',\n",
    "        tickformat= \"%Y\",\n",
    "        range = ['1994', '2000']\n",
    "                ),\n",
    "    legend=dict(\n",
    "        x=0.3,y=1,\n",
    "        bordercolor='#00000f',\n",
    "            borderwidth=2\n",
    "\n",
    "\n",
    "                ),\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    margin=go.Margin(l=300, b=150),\n",
    "    #paper_bgcolor='rgb(233,233,233)',\n",
    "    #plot_bgcolor='rgb(233,233,233)',\n",
    "    \n",
    "    \n",
    "    font=dict( size=35)\n",
    "             )\n",
    "             \n",
    "# create the figure object            \n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "# plot the figure \n",
    "#py.iplot(fig, filename = \"2.2bIdentify_aggregate_TimeSeriesValues\")       \n",
    "\n",
    "\n",
    "## it can be run from the local machine on Pycharm like this like below\n",
    "## It would also work here offline but in a seperate window  \n",
    "offline.iplot(fig,filename = 'UseCase3.1b_TimeSeries')#,image='png' )       \n",
    "print \"the plot is generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"PickaSource\"></a>\n",
    "# 6. Pick a a flow source and update the db to reflect \"Verified\"\n",
    "\n",
    "This \"Update\" SQL query allows users to update the Mappings table to indicate a \"verified\" DataValue. \n",
    "A verified record set to True indicates that the user has verified, curated, checked, or selected this \n",
    "data value as ready to be used for models. A verified record can then be used from an automated script to \n",
    "serve data to models. Its particularly useful when the same set of controlled object type, attribute, and instances names \n",
    "return multiple data values from different sources with potentially similar or different values due to many factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. Pick a a flow source and update the db to reflect \"Verified\"\n",
    "\n",
    "# scenario_name_data = subsets.get_group(name='Base case')\n",
    "# print scenario_name_data\n",
    "# Get a cursor object\n",
    "\n",
    "SQL_update = \"\"\"\n",
    "UPDATE Mappings \n",
    "\n",
    "SET Verified= 'True'\n",
    "WHERE  MappingID in\n",
    "\n",
    "(SELECT Mappings.MappingID FROM Mappings\n",
    "\n",
    "-- Join the Mappings to get their Attributes\n",
    "LEFT JOIN \"Attributes\"\n",
    "ON Attributes.AttributeID= Mappings.AttributeID\n",
    "\n",
    "-- Join the Attributes to get their ObjectTypes\n",
    "LEFT JOIN  \"ObjectTypes\"\n",
    "ON \"ObjectTypes\".\"ObjectTypeID\"=\"Attributes\".\"ObjectTypeID\"\n",
    "\n",
    "-- Join the Mappings to get their Instances   \n",
    "LEFT JOIN \"Instances\" \n",
    "ON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n",
    "\n",
    "-- Join the Mappings to get their ScenarioMappings   \n",
    "LEFT JOIN \"ScenarioMappings\"\n",
    "ON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n",
    "\n",
    "-- Join the ScenarioMappings to get their Scenarios   \n",
    "LEFT JOIN \"Scenarios\"\n",
    "ON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n",
    "\n",
    "-- Join the Scenarios to get their MasterNetworks   \n",
    "LEFT JOIN \"MasterNetworks\" \n",
    "ON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n",
    "\n",
    "where \n",
    "ObjectTypes.ObjectType='Site'  \n",
    "\n",
    "AND \"Instances\".\"InstanceName\"=\"10046500.MONBEAR RIVER BL STEWART DAM NR MONTPELIER IDAHO\"  \n",
    "\n",
    "AND AttributeName='Delivered volume per month'\n",
    "\n",
    "AND ScenarioName='Existing data'\n",
    "\n",
    "AND MasterNetworkName='UDWRFlowData')\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "res = cur.execute(SQL_update)\n",
    "\n",
    "print 'updated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Close\"></a>\n",
    "# 7. Close the SQLite connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()\n",
    "\n",
    "print 'Connection to SQLite engine is disconnected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End :) Congratulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
