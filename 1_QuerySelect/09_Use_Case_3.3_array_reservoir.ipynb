{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaMDaM_Use_Case 3.3: What reservoir volume-elevation curve to use in a model?\n",
    "\n",
    "#### By Adel M. Abdallah, Utah State University, August 2018\n",
    "\n",
    "This notebook demonstrates basic WaMDaM use cases analysis using scientific Python libraries such as [pandas](https://pandas.pydata.org/) and [plotly](https://plot.ly/).  It reads WaMDaM SQLite, runs SQL script, and them uses Python plotly to visualize the results\n",
    "\n",
    "\n",
    "Execute the following cells by pressing `Shift-Enter`, or by pressing the play button \n",
    "<img style='display:inline;padding-bottom:15px' src='play-button.png'>\n",
    "on the toolbar above.\n",
    "\n",
    "\n",
    "\n",
    "EDIT THESE LINES\n",
    "This use case identifies five time series and seasonal flow data for the site below Stewart Dam, Idaho\n",
    "\n",
    "\n",
    "\n",
    "### Steps to reproduce this use case results and plots \n",
    "\n",
    "1.[Import python libraries](#Import)   \n",
    "   \n",
    "   \n",
    "2.[Connect to the WaMDaM populated SQLite file](#Connect)    \n",
    " \n",
    " \n",
    "3.[Query the multi-column array: Reservoir Bathymetry](#QueryBathymetry)   \n",
    "  \n",
    "\n",
    "4.[plot the multi-column array: Reservoir Bathymetry](#Plot)  \n",
    "\n",
    "\n",
    "5.[Pick a a flow source and update the db to reflect \"Verified\"](#PickaSource)  \n",
    "  \n",
    "  \n",
    "6.[Close the SQLite and WEAP API connections](#Close)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Import\"></a>\n",
    "# 1. Import python libraries \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Import python libraries \n",
    "### set the notebook mode to embed the figures within the cell\n",
    "\n",
    "import plotly\n",
    "plotly.__version__\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "offline.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "init_notebook_mode(connected=True)         # initiate notebook for offline plot\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image, SVG, Math, YouTubeVideo\n",
    "import urllib\n",
    "\n",
    "print 'The needed Python libraries have been imported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Connect\"></a>\n",
    "# 2. Connect to the WaMDaM populated SQLite file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Then we can run queries against it within this notebook :)  \n",
    "\n",
    "# the SQLite file is published here \n",
    "\n",
    "\n",
    "WaMDaM_SQLite_Name='BearRiverDatasets_June_2018_Final.sqlite'\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(WaMDaM_SQLite_Name)\n",
    "\n",
    "print 'Connected to the WaMDaM SQLite file called'+': '+ WaMDaM_SQLite_Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"QueryBathymetry\"></a>\n",
    "# 3. Query the multi-column array: Reservoir Bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '/*\n-- 4.2 MultiAttributeValues.sql\n\nUse case 4: identify and compare infrastructure data across many data sources. \nWhat is the volume, purpose, evaporation, and elevation of Hyrum Reservoir Utah?\n\n\nAdel Abdallah\nUpdated April 3, 2018\n\nThis query shows data values for a particular MultiColumns of a reservoir InstanceName: area, and capacity, and stage \n\nResult:\nUsers can import these data columns to their model \nWaM-DaM keeps track of the meanings of data values, their CV_Units, to what instance they apply too.... \n*/\n\nSELECT \"ObjectTypes\".\"ObjectType\",\n\"Instances\".\"InstanceName\",ScenarioName,\"Attributes\".\"AttributeName\" AS MultiAttributeName,\"Attributes\".AttributeDataTypeCV,\n\"AttributesColumns\".\"AttributeName\" AS \"AttributeName\",\n\"AttributesColumns\".\"AttributeNameCV\",\n\"AttributesColumns\".\"UnitNameCV\" AS \"AttributeNameUnitName\",\n\"DataValue\",\"ValueOrder\"\n\nFROM ResourceTypes\n\nLeft JOIN \"ObjectTypes\" \nON \"ObjectTypes\".\"ResourceTypeID\"=\"ResourceTypes\".\"ResourceTypeID\"\n\n-- Join the Object types to get their attributes  \nLEFT JOIN  \"Attributes\"\nON \"Attributes\".\"ObjectTypeID\"=\"ObjectTypes\".\"ObjectTypeID\"\n\n-- Join the Attributes to get their Mappings   \nLEFT JOIN \"Mappings\"\nON Mappings.AttributeID= Attributes.AttributeID\n\n-- Join the Mappings to get their Instances   \nLEFT JOIN \"Instances\" \nON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n\n-- Join the Mappings to get their ScenarioMappings   \nLEFT JOIN \"ScenarioMappings\"\nON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n\n-- Join the ScenarioMappings to get their Scenarios   \nLEFT JOIN \"Scenarios\"\nON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n\n\n-- Join the Scenarios to get their MasterNetworks   \nLEFT JOIN \"MasterNetworks\" \nON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n\n-- Join the Mappings to get their Methods   \nLEFT JOIN \"Methods\" \nON \"Methods\".\"MethodID\"=\"Mappings\".\"MethodID\"\n\n-- Join the Mappings to get their Sources   \nLEFT JOIN \"Sources\" \nON \"Sources\".\"SourceID\"=\"Mappings\".\"SourceID\"\n\n-- Join the Mappings to get their DataValuesMappers   \nLEFT JOIN \"ValuesMapper\" \nON \"ValuesMapper\".\"ValuesMapperID\"=\"Mappings\".\"ValuesMapperID\"\n\n-- Join the DataValuesMapper to get their MultiAttributeSeries   \nLEFT JOIN \"MultiAttributeSeries\"  \nON \"MultiAttributeSeries\" .\"ValuesMapperID\"=\"ValuesMapper\".\"ValuesMapperID\"\n\n\n/*This is an extra join to get to each column name within the MultiColumn Array */\n\n-- Join the MultiAttributeSeries to get to their specific DataValuesMapper, now called DataValuesMapperColumn\nLEFT JOIN \"ValuesMapper\" As \"ValuesMapperColumn\"\nON \"ValuesMapperColumn\".\"ValuesMapperID\"=\"MultiAttributeSeries\".\"MappingID_Attribute\"\n\n-- Join the DataValuesMapperColumn to get back to their specific Mapping, now called MappingColumns\nLEFT JOIN \"Mappings\" As \"MappingColumns\"\nON \"MappingColumns\".\"ValuesMapperID\"=\"ValuesMapperColumn\".\"ValuesMapperID\"\n\n-- Join the MappingColumns to get back to their specific Attribute, now called AttributeColumns\nLEFT JOIN  \"Attributes\" AS \"AttributesColumns\"\nON \"AttributesColumns\".\"AttributeID\"=\"MappingColumns\".\"AttributeID\"\n/* Finishes here */\n\n-- Join the MultiAttributeSeries to get access to their MultiAttributeSeriesValues   \nLEFT JOIN \"MultiAttributeSeriesValues\"\nON \"MultiAttributeSeriesValues\".\"MultiAttributeSeriesID\"=\"MultiAttributeSeries\".\"MultiAttributeSeriesID\"\n\n-- Select one InstanceName and restrict the query AttributeDataTypeCV that is MultiAttributeSeries   \n\nWHERE  Attributes.AttributeDataTypeCV='MultiAttributeSeries' \n\n\nAND \"Instances\".\"InstanceNameCV\"='Hyrum Reservoir'  \n\nAND (\"AttributesColumns\".\"AttributeNameCV\" ='Volume' or \"AttributesColumns\".\"AttributeNameCV\" ='Elevation' )\n\n--AND ScenarioName='Reference_LowerBear'\n\n-- Sort the the values of each column name based on their ascending order\nORDER BY ResourceTypeAcronym,ObjectType,InstanceName,ScenarioName,AttributeName,MultiAttributeName,ValueOrder ASC\n': no such table: ResourceTypes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ed03793983f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;31m# return query result in a pandas data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mresult_df_UseCase3_3b\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQuery_UseCase3_3b_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;31m# uncomment the below line to see the list of attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    330\u001b[0m     return pandas_sql.read_query(\n\u001b[1;32m    331\u001b[0m         \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         parse_dates=parse_dates, chunksize=chunksize)\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             ex = DatabaseError(\n\u001b[1;32m   1415\u001b[0m                 \"Execution failed on sql '%s': %s\" % (args[0], exc))\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Adel\\AppData\\Roaming\\Python\\Python27\\site-packages\\pandas\\io\\sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '/*\n-- 4.2 MultiAttributeValues.sql\n\nUse case 4: identify and compare infrastructure data across many data sources. \nWhat is the volume, purpose, evaporation, and elevation of Hyrum Reservoir Utah?\n\n\nAdel Abdallah\nUpdated April 3, 2018\n\nThis query shows data values for a particular MultiColumns of a reservoir InstanceName: area, and capacity, and stage \n\nResult:\nUsers can import these data columns to their model \nWaM-DaM keeps track of the meanings of data values, their CV_Units, to what instance they apply too.... \n*/\n\nSELECT \"ObjectTypes\".\"ObjectType\",\n\"Instances\".\"InstanceName\",ScenarioName,\"Attributes\".\"AttributeName\" AS MultiAttributeName,\"Attributes\".AttributeDataTypeCV,\n\"AttributesColumns\".\"AttributeName\" AS \"AttributeName\",\n\"AttributesColumns\".\"AttributeNameCV\",\n\"AttributesColumns\".\"UnitNameCV\" AS \"AttributeNameUnitName\",\n\"DataValue\",\"ValueOrder\"\n\nFROM ResourceTypes\n\nLeft JOIN \"ObjectTypes\" \nON \"ObjectTypes\".\"ResourceTypeID\"=\"ResourceTypes\".\"ResourceTypeID\"\n\n-- Join the Object types to get their attributes  \nLEFT JOIN  \"Attributes\"\nON \"Attributes\".\"ObjectTypeID\"=\"ObjectTypes\".\"ObjectTypeID\"\n\n-- Join the Attributes to get their Mappings   \nLEFT JOIN \"Mappings\"\nON Mappings.AttributeID= Attributes.AttributeID\n\n-- Join the Mappings to get their Instances   \nLEFT JOIN \"Instances\" \nON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n\n-- Join the Mappings to get their ScenarioMappings   \nLEFT JOIN \"ScenarioMappings\"\nON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n\n-- Join the ScenarioMappings to get their Scenarios   \nLEFT JOIN \"Scenarios\"\nON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n\n\n-- Join the Scenarios to get their MasterNetworks   \nLEFT JOIN \"MasterNetworks\" \nON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n\n-- Join the Mappings to get their Methods   \nLEFT JOIN \"Methods\" \nON \"Methods\".\"MethodID\"=\"Mappings\".\"MethodID\"\n\n-- Join the Mappings to get their Sources   \nLEFT JOIN \"Sources\" \nON \"Sources\".\"SourceID\"=\"Mappings\".\"SourceID\"\n\n-- Join the Mappings to get their DataValuesMappers   \nLEFT JOIN \"ValuesMapper\" \nON \"ValuesMapper\".\"ValuesMapperID\"=\"Mappings\".\"ValuesMapperID\"\n\n-- Join the DataValuesMapper to get their MultiAttributeSeries   \nLEFT JOIN \"MultiAttributeSeries\"  \nON \"MultiAttributeSeries\" .\"ValuesMapperID\"=\"ValuesMapper\".\"ValuesMapperID\"\n\n\n/*This is an extra join to get to each column name within the MultiColumn Array */\n\n-- Join the MultiAttributeSeries to get to their specific DataValuesMapper, now called DataValuesMapperColumn\nLEFT JOIN \"ValuesMapper\" As \"ValuesMapperColumn\"\nON \"ValuesMapperColumn\".\"ValuesMapperID\"=\"MultiAttributeSeries\".\"MappingID_Attribute\"\n\n-- Join the DataValuesMapperColumn to get back to their specific Mapping, now called MappingColumns\nLEFT JOIN \"Mappings\" As \"MappingColumns\"\nON \"MappingColumns\".\"ValuesMapperID\"=\"ValuesMapperColumn\".\"ValuesMapperID\"\n\n-- Join the MappingColumns to get back to their specific Attribute, now called AttributeColumns\nLEFT JOIN  \"Attributes\" AS \"AttributesColumns\"\nON \"AttributesColumns\".\"AttributeID\"=\"MappingColumns\".\"AttributeID\"\n/* Finishes here */\n\n-- Join the MultiAttributeSeries to get access to their MultiAttributeSeriesValues   \nLEFT JOIN \"MultiAttributeSeriesValues\"\nON \"MultiAttributeSeriesValues\".\"MultiAttributeSeriesID\"=\"MultiAttributeSeries\".\"MultiAttributeSeriesID\"\n\n-- Select one InstanceName and restrict the query AttributeDataTypeCV that is MultiAttributeSeries   \n\nWHERE  Attributes.AttributeDataTypeCV='MultiAttributeSeries' \n\n\nAND \"Instances\".\"InstanceNameCV\"='Hyrum Reservoir'  \n\nAND (\"AttributesColumns\".\"AttributeNameCV\" ='Volume' or \"AttributesColumns\".\"AttributeNameCV\" ='Elevation' )\n\n--AND ScenarioName='Reference_LowerBear'\n\n-- Sort the the values of each column name based on their ascending order\nORDER BY ResourceTypeAcronym,ObjectType,InstanceName,ScenarioName,AttributeName,MultiAttributeName,ValueOrder ASC\n': no such table: ResourceTypes"
     ]
    }
   ],
   "source": [
    "# Use Case 3.1Identify_aggregate_TimeSeriesValues.csv\n",
    "# plot aggregated to monthly and converted to acre-feet time series data of multiple sources\n",
    "\n",
    "\n",
    "# display (df_Seasonal)\n",
    "\n",
    "\n",
    "# column_name = \"InstanceName\"\n",
    "# subsets = df_Seasonal.groupby(column_name)\n",
    "\n",
    "\n",
    "# 4.3MergeTimeSeriesValues\n",
    "Query_UseCase3_3b_URL=\"\"\"\n",
    "https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/4_Queries_SQL/UseCase3/UseCase3.3/4_MultiAttributeValues.sql\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Read the query text inside the URL\n",
    "Query_UseCase3_3b_text = urllib.urlopen(Query_UseCase3_3b_URL).read()\n",
    "\n",
    "\n",
    "# return query result in a pandas data frame\n",
    "result_df_UseCase3_3b= pd.read_sql_query(Query_UseCase3_3b_text, conn)\n",
    "\n",
    "# uncomment the below line to see the list of attributes\n",
    "# display (result_df_required)\n",
    "\n",
    "\n",
    "# Save the datafrom as a csv file into the Jupyter notebook working space\n",
    "# result_df_UseCase3_3b.to_csv('UseCases_Results_csv\\result_df_UseCase3_3b.csv', index = False)\n",
    "\n",
    "df=result_df_UseCase3_3b\n",
    "# result_df_UseCase3_3b.to_csv('result_df_UseCase3_3b.csv', index = False)\n",
    "\n",
    "# display (df)\n",
    "\n",
    "\n",
    "\n",
    "# cur = conn.cursor()\n",
    "# data = cur.execute(Query_UseCase3_3b_text)\n",
    "\n",
    "# print data\n",
    "\n",
    "# df = df.to_csv(header=True, index=False).strip('\\n').split('\\n')\n",
    "# df=df.values.tolist()\n",
    "\n",
    "# xx=df.to_csv(header=False, index=False)\n",
    "# display (xx)\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\n",
    "# 4.3MergeTimeSeriesValues\n",
    "Query_UseCase3_3a_URL=\"\"\"\n",
    "https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/4_Queries_SQL/UseCase3/UseCase3.3/3_MergeTimeSeriesValues.sql\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Read the query text inside the URL\n",
    "Query_UseCase3_3a_text = urllib.urlopen(Query_UseCase3_3a_URL).read()\n",
    "\n",
    "\n",
    "# return query result in a pandas data frame\n",
    "result_df_UseCase3_3a= pd.read_sql_query(Query_UseCase3_3a_text, conn)\n",
    "\n",
    "# result_df_UseCase3_3a.to_csv('result_df_UseCase3_3a.csv', index = False)\n",
    "\n",
    "\n",
    "# uncomment the below line to see the list of attributes\n",
    "# display (result_df_required)\n",
    "\n",
    "\n",
    "# Save the datafrom as a csv file into the Jupyter notebook working space\n",
    "# result_df_UseCase3_3a.to_csv('UseCases_Results_csv\\result_df_UseCase3_3a.csv', index = False)\n",
    "\n",
    "df2=result_df_UseCase3_3a\n",
    "# df2 = df2.to_csv(header=True, index=False).strip('\\n').split('\\n')\n",
    "\n",
    "# display (df2)\n",
    "\n",
    "print '####################################'\n",
    "print 'query is done'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Plot\"></a>\n",
    "# 4. Plot the multi-column array: Reservoir Bathymetry\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UseCase2.3_HyrumReservoir_Curves.py\n",
    "\n",
    "# # 4.2MultiAttributeValues.csv\n",
    "# df = pd.read_csv(\"https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/UseCases_files/5Results_CSV/4.2MultiAttributeValues.csv\")\n",
    "\n",
    "# # # 4.3MergeTimeSeriesValues.sql\n",
    "# df2 = pd.read_csv(\"https://raw.githubusercontent.com/WamdamProject/WaMDaM_UseCases/master/UseCases_files/5Results_CSV/4.3MergeTimeSeriesValues.csv\")\n",
    "\n",
    "# display (df)\n",
    "\n",
    "\n",
    "subsets = df.groupby('ScenarioName')\n",
    "data = []\n",
    "\n",
    "#for each subset (curve), set up its legend and line info manually so they can be edited\n",
    "subsets_settings = {\n",
    "    'Utah Dams shapefile_as is': { # this oone is the name of subset as it appears in the csv file\n",
    "        'dash': 'solid',     # this is properity of the line (curve)\n",
    "        'width':3,\n",
    "        'legend_index': 1,   # to order the legend\n",
    "         'symbol':'square',\n",
    "        'size':7,\n",
    "        'mode':'line+markers',\n",
    "        'legend_name': 'Utah Dams Dataset (2016)',  # this is the manual curve name \n",
    "         'color':'#990F0F'\n",
    "        },\n",
    "    \n",
    "    'USU WEAP Model 2017': {\n",
    "        'dash': 'solid',\n",
    "         'width':3,\n",
    "          'mode':'line+markers',\n",
    "          'symbol':'circle',\n",
    "                'size':7,\n",
    "\n",
    "        'legend_index': 3,\n",
    "        'legend_name': 'USU WEAP Model (2017)',\n",
    "         'color':'#B26F2C'\n",
    "        },\n",
    "    'USU WEAP Model 2010': {\n",
    "        'dash': 'dash',\n",
    "        'mode':'line+markers',\n",
    "        'width':3,\n",
    "                'size':7,\n",
    "\n",
    "        'symbol':'circle',\n",
    "        'legend_index': 4,\n",
    "        'legend_name': 'USU WEAP Model 2010',\n",
    "         'color':'#7A430C'\n",
    "        },\n",
    "    'Rwise': {\n",
    "        'dash': 'dash',\n",
    "        'mode':'line+markers',\n",
    "        'width':3,\n",
    "                  'symbol':'star',\n",
    "                'size':7,\n",
    "\n",
    "        'legend_index': 0,\n",
    "        'legend_name': 'BOR Water Info. System (2017)',\n",
    "         'color':'#E57E7E'\n",
    "        },\n",
    "    'Base case': {\n",
    "        'dash': 'solid',\n",
    "        'mode':'lines+markers',\n",
    "        'width':3,\n",
    "                  'symbol':'bowtie',\n",
    "        'size':11,\n",
    "\n",
    "        'legend_index': 2,\n",
    "        'legend_name': 'BOR Reservoirs Dataset (2006)',\n",
    "         'color':'#E5B17E'\n",
    "        },    \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# This dict is used to map legend_name to original subset name\n",
    "subsets_names = {y['legend_name']: x for x,y in subsets_settings.iteritems()}\n",
    "\n",
    "      \n",
    "#for each subset (curve), set up its legend and line info manually so they can be edited\n",
    "subsets_settings2 = {\n",
    "        'dash': 'solid',     # this is properity of the line (curve)\n",
    "        'legend_index': 3,   # to order the legend\n",
    "         'mode':'lines+markers',\n",
    "        'color':'#E57E7E',\n",
    "        'legend_name': 'BOR Water Info. System (2017)'  # this is the edited curve name \n",
    "                    }\n",
    "\n",
    "\n",
    "# Get data from first dataframe (Multi-Attributes)\n",
    "for subset in subsets.groups.keys():\n",
    "#     print subset\n",
    "    name = subsets_settings[subset]['legend_name']\n",
    "    print name\n",
    "    scenario_name_data = subsets.get_group(name=subset)\n",
    "    subsets_of_scenario = scenario_name_data.groupby(\"AttributeNameCV\")\n",
    "    s = go.Scatter(\n",
    "                    x=subsets_of_scenario.get_group(name='Volume').DataValue,\n",
    "                    y=subsets_of_scenario.get_group(name='Elevation').DataValue,\n",
    "                        mode='lines+markers',\n",
    "\n",
    "                    name = subsets_settings[subset]['legend_name'],\n",
    "                    line = dict(\n",
    "                        color =subsets_settings[subset]['color'],\n",
    "                        width =subsets_settings[subset]['width'],\n",
    "                        dash=subsets_settings[subset]['dash']\n",
    "                                ),\n",
    "                     marker = dict(\n",
    "                         size=subsets_settings[subset]['size'],\n",
    "                         symbol=subsets_settings[subset]['symbol'],\n",
    "                         #color = '#a50021',\n",
    "                         maxdisplayed=12\n",
    "),  \n",
    "                    opacity = 1)\n",
    "    data.append(s)\n",
    "\n",
    "\n",
    "# Get data from second dataframe (merged two time series as two Multi-Attributes)\n",
    "data2 = go.Scatter(\n",
    "                x=df2.VolumeValue,\n",
    "                y=df2['ElevationValue'],\n",
    "                name = subsets_settings2['legend_name'],\n",
    "                mode='lines+markers',\n",
    "                line = dict(\n",
    "                    color ='#E57E7E',\n",
    "                    width =3),\n",
    "                marker = dict(\n",
    "                size =9,\n",
    "                color = '#E57E7E',\n",
    "                maxdisplayed=20,\n",
    "                symbol ='star',\n",
    "                         line = dict(\n",
    "                         color = ['rgb(153, 84, 15)']\n",
    "                         ),\n",
    "\n",
    "                            ),\n",
    "    \n",
    "    \n",
    "                opacity =1)\n",
    "                \n",
    "data.append(data2)     \n",
    "    \n",
    "# Legend is ordered based on data, so we are sorting the data based \n",
    "# on desired legend order indicarted by the index value entered above\n",
    "data.sort(key=lambda x: subsets_settings[subsets_names[x['name']]]['legend_index'])\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=[1500, 8000, 16000],\n",
    "    y=[4680, 4680,4680],\n",
    "    mode='text',\n",
    "    showlegend=False,\n",
    "    text=['Dead<br> storage', 'Live<br>storage', 'Total<br>storage'],\n",
    "    textposition='top center',\n",
    "\n",
    ")\n",
    "data.append(trace1)     \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "layout = {\n",
    "        'shapes': [\n",
    "        # Rectangle reference to the axes\n",
    "        {\n",
    "            \"opacity\": 0.3,\n",
    "            'type': 'rect',\n",
    "            'xref': 'x',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': 4580,\n",
    "            'x1': 3012,\n",
    "            'y1': 4750,\n",
    "            'line': {\n",
    "                'color': 'rgb(0, 0, 0)',\n",
    "                'width': 0.1,\n",
    "            },\n",
    "            'fillcolor': 'rgb(153, 229, 255)'\n",
    "        },\n",
    "     # Rectangle reference to the plot\n",
    "        {\n",
    "           \"opacity\": 0.3,\n",
    "            'type': 'rect',\n",
    "            'xref': 'x',\n",
    "            'yref': 'y',\n",
    "            'x0': 3012,\n",
    "            'y0': 4580,\n",
    "            'x1': 14440,\n",
    "            'y1': 4750,\n",
    "            'line': {\n",
    "                'color': 'rgb(0, 0, 0)',\n",
    "                'width': 0.1,\n",
    "            },\n",
    "            'fillcolor': 'rgb(127, 212, 255)',\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"opacity\": 0.3,\n",
    "            'type': 'rect',\n",
    "            'xref': 'x',\n",
    "            'yref': 'y',\n",
    "            'x0': 14440,\n",
    "            'y0': 4580,\n",
    "            'x1': 17746,\n",
    "            'y1': 4750,\n",
    "            'line': {\n",
    "                'color': 'rgb(0, 0, 0)',\n",
    "                'width': 0.1,\n",
    "            },\n",
    "            'fillcolor': 'rgb(101, 191, 255)',\n",
    "        }        \n",
    "    ],\n",
    "        'yaxis': {\n",
    "        'title': 'Elevation (feet)',\n",
    "        'tickformat': ',',\n",
    "        'ticks':'outside',\n",
    "        'ticklen': 10,\n",
    "\n",
    "        'showline':True,\n",
    "\n",
    "        'range' : ['4580', '4700'],\n",
    "                'showline':True\n",
    "\n",
    "                },\n",
    "    'xaxis' : {\n",
    "        'title' : 'Volume (acre-feet)',\n",
    "        'tickformat': ',',   \n",
    "         'showgrid':False,\n",
    "\n",
    "        'ticks':'outside',\n",
    "        'dtick':5000,\n",
    "        'range' : ['0', '30000'],\n",
    "        'ticklen':20,\n",
    "        'tick0':0,\n",
    "        'showline':True,\n",
    "    },\n",
    "    'legend':{\n",
    "        'x':0.45,\n",
    "        'y':0.04,\n",
    "        'bordercolor':'#00000f',\n",
    "         'borderwidth':2    \n",
    "    },\n",
    "    'width':1200,\n",
    "    'height':800,\n",
    "    'margin':go.Margin(\n",
    "        l=150,\n",
    "        b=150       ),\n",
    "    #paper_bgcolor='rgb(233,233,233)',\n",
    "    #plot_bgcolor='rgb(233,233,233)',\n",
    "    'font':{'size':32,'family':'arial'},\n",
    "    \n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "    #title = \"UseCase5\",\n",
    "\n",
    "annotations=[\n",
    "    dict(\n",
    "        x=17700,\n",
    "        y=4673,\n",
    "        xref='x',\n",
    "        yref='y',\n",
    "        text='Selected',\n",
    "#         size=15,\n",
    "        showarrow=True,\n",
    "#         arrowhead=100,\n",
    "        ax=100,\n",
    "        ay=-100\n",
    "    )\n",
    "]\n",
    "\n",
    "layout['annotations'] = annotations\n",
    "# set up the figure layout\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "fig = {\n",
    "    'data': data,\n",
    "    'layout': layout}\n",
    "\n",
    "\n",
    "\n",
    "#py.iplot(fig, filename = \"4_HyrumReservoir_Curves.py\") \n",
    "\n",
    "\n",
    "## it can be run from the local machine on Pycharm like this like below\n",
    "## It would also work here offline but in a seperate window  \n",
    "#plotly.offline.plot(fig, filename = \"4_HyrumReservoir_Curves.py\") offline.iplot(fig,filename = 'jupyter/2.2Identify_aggregate_TimeSeriesValues' )       \n",
    "\n",
    "offline.iplot(fig,filename = 'UseCase_3_3_HyrumReservoir_Curves')#,image='png' )       \n",
    "\n",
    "# Save the figure as a png image:\n",
    "# use plotly.offline.iplot for offline plot\n",
    "\n",
    "#offline.iplot(fig,filename = 'jupyter/4_HyrumReservoir_Curves',\n",
    "             #image='png')\n",
    "# it might take 30-60 seconds to load the html interactive image \n",
    "print \"the plot is generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"PickaSource\"></a>\n",
    "# 5. Pick a a flow source and update the db to reflect \"Verified\"\n",
    "\n",
    "This \"Update\" SQL query allows users to update the Mappings table to indicate a \"verified\" DataValue. \n",
    "A verified record set to True indicates that the user has verified, curated, checked, or selected this \n",
    "data value as ready to be used for models. A verified recorded can then be used from an automated script to \n",
    "serve data to models. Its particularly useful when the same set of controlled object type, attribute, and instances names \n",
    "return multiple data values from different sources with potentially similar or different values due to many factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5. Pick a a flow source and update the db to reflect \"Verified\"\n",
    "\n",
    "\n",
    "#scenario_name_data = subsets.get_group(name='Base case')\n",
    "#print scenario_name_data\n",
    "\n",
    "SQL_update = \"\"\"\n",
    "\n",
    "UPDATE Mappings \n",
    "\n",
    "SET Verified= 'True'\n",
    "WHERE  MappingID in\n",
    "\n",
    "(\n",
    "SELECT Mappings.MappingID FROM Mappings\n",
    "\n",
    "-- Join the Mappings to get their Attributes\n",
    "LEFT JOIN \"Attributes\"\n",
    "ON Attributes.AttributeID= Mappings.AttributeID\n",
    "\n",
    "-- Join the Attributes to get their ObjectTypes\n",
    "LEFT JOIN  \"ObjectTypes\"\n",
    "ON \"ObjectTypes\".\"ObjectTypeID\"=\"Attributes\".\"ObjectTypeID\"\n",
    "\n",
    "-- Join the Mappings to get their Instances   \n",
    "LEFT JOIN \"Instances\" \n",
    "ON \"Instances\".\"InstanceID\"=\"Mappings\".\"InstanceID\"\n",
    "\n",
    "-- Join the Mappings to get their ScenarioMappings   \n",
    "LEFT JOIN \"ScenarioMappings\"\n",
    "ON \"ScenarioMappings\".\"MappingID\"=\"Mappings\".\"MappingID\"\n",
    "\n",
    "-- Join the ScenarioMappings to get their Scenarios   \n",
    "LEFT JOIN \"Scenarios\"\n",
    "ON \"Scenarios\".\"ScenarioID\"=\"ScenarioMappings\".\"ScenarioID\"\n",
    "\n",
    "-- Join the Scenarios to get their MasterNetworks   \n",
    "LEFT JOIN \"MasterNetworks\" \n",
    "ON \"MasterNetworks\".\"MasterNetworkID\"=\"Scenarios\".\"MasterNetworkID\"\n",
    "\n",
    "where \n",
    "ObjectTypes.ObjectType='Reservoir'  \n",
    "\n",
    "AND \"Instances\".\"InstanceName\"='Hyrum Reservoir'  \n",
    "\n",
    "AND AttributeName='Total Capacity Table'\n",
    "\n",
    "AND ScenarioName='Base case'\n",
    "\n",
    "AND MasterNetworkName='Hyrum'\n",
    "\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "res = cur.execute(SQL_update)\n",
    "\n",
    "print 'updated'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Close\"></a>\n",
    "# 6. Close the SQLite and WEAP API connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()\n",
    "\n",
    "print 'Connection to SQLite engine is disconnected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End :) Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
